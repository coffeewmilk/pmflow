services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    hostname: zookeeper 
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
        ZOOKEEPER_CLIENT_PORT: 2181
    healthcheck:
      test: ["CMD-SHELL", "nc localhost 2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  
  broker:
    image: confluentinc/cp-kafka
    hostname: broker 
    container_name: broker
    depends_on:
        zookeeper:
          condition: 'service_healthy'
    ports:
      - "29092:29092"
    environment:
        KAFKA_BROKER_ID: 1
        KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
        KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL_SAME_HOST:PLAINTEXT
        KAFKA_LISTENERS: EXTERNAL_SAME_HOST://:29092,INTERNAL://:9092
        KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker:9092, EXTERNAL_SAME_HOST://localhost:29092
        KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
        KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
        KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema_registry:8081

  connector:
    image: confluentinc/cp-kafka-connect-base
    command: 
      - bash 
      - -c
      - confluent-hub install --no-prompt confluentinc/kafka-connect-s3:latest
      - confluent-hub install --no-prompt confluentinc/kafka-connect-hdfs3:latest && /etc/confluent/docker/run && sleep infinity

    hostname: connector
    container_name: connector
    user: root
    ports:
      - "8083:8083"
    depends_on:
      - broker
    volumes:
      - $PWD/credentials:/root/.aws/credentials
      - $PWD/connectors:/connectors
    environment:
        CONNECT_BOOTSTRAP_SERVERS: 'broker:9092'
        CONNECT_REST_PORT: 8083
        CONNECT_GROUP_ID: "quickstart"
        CONNECT_CONFIG_STORAGE_TOPIC: "quickstart-config"
        CONNECT_OFFSET_STORAGE_TOPIC: "quickstart-offsets"
        CONNECT_STATUS_STORAGE_TOPIC: "quickstart-status"
        CONNECT_KEY_CONVERTER: "io.confluent.connect.avro.AvroConverter"
        CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
        CONNECT_VALUE_CONVERTER: "io.confluent.connect.avro.AvroConverter"
        CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
        CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
        CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
        CONNECT_REST_ADVERTISED_HOST_NAME: localhost
        CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components
        CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
        CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
        CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
  

  schema-registry:
    image: confluentinc/cp-schema-registry
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - broker
    ports:
      - "8081:8081"
    volumes:
      - $PWD/avro:/avro
    environment:
        SCHEMA_REGISTRY_HOST_NAME: schema-registry
        SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'broker:9092'
        SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'
      

  alpine:
    image: alpine:latest
    container_name: alpine
    working_dir: /scripts
    depends_on:
      - schema-registry
      - connector
    volumes:
      - $PWD/avro:/scripts/avro
      - $PWD/connectors:/scripts/connectors
    command:
      - sh 
      - -c 
      - |
        apk update --no-cache 
        apk --no-cache add curl
        apk add --no-cache wget openssl 
        wget -O - https://github.com/jwilder/dockerize/releases/download/v0.7.0/dockerize-linux-amd64-v0.7.0.tar.gz | tar xzf - -C /usr/local/bin 
        apk del wget
        dockerize -wait http://connector:8083  - wait http://schema-registry:8081 -timeout 2m
        sh ./avro/init_schema.sh
        sh ./connectors/init_s3_connector.sh


  pyspark:
    image: bitnami/spark:3.5.0
    container_name: spark
    working_dir: /usr/src/app
    user: root
    depends_on:
      - broker
      - alpine
    volumes:
      - $PWD/requirements.txt:/usr/src/app/requirements.txt
      - $PWD/kafka:/usr/src/app/kafka
      - $PWD/avro:/usr/src/app/avro
    command: 
      - bash 
      - -c 
      - |
        apt update && apt install -y openjdk-17-jdk
        pip install --no-cache-dir -r requirements.txt
        spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,\
        org.apache.spark:spark-avro_2.12:3.5.0,\
        org.apache.sedona:sedona-spark-shaded-3.5_2.12:1.5.1,\
        org.datasyslab:geotools-wrapper:1.5.1-28.2,\
        edu.ucar:cdm-core:5.4.2,\
        za.co.absa:abris_2.12:6.4.0\
                     --repositories https://artifacts.unidata.ucar.edu/content/repositories/unidata-releases,https://repo1.maven.org/maven2,https://packages.confluent.io/maven \
                      ./kafka/pmflow_avro.py

  namenode:
    image: apache/hadoop:3.3.6
    platform: linux/amd64
    container_name: namenode
    hostname: namenode
    command: 
      - bash 
      - -c 
      - hdfs namenode
    ports:
      - 9870:9870
      - 50070:50070
    env_file:
      - ./hadoop/config
    environment:
        ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
  
  datanode:
    image: apache/hadoop:3.3.6
    platform: linux/amd64
    container_name: datanode
    command: ["hdfs", "datanode"]
    depends_on:
      - namenode
    env_file:
      - ./hadoop/config  
  
